{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(url: \"https://github.com/mxcl/Path.swift\", from: \"1.2.0\")\n",
      "\t\tPath\n",
      "\t.package(url: \"https://github.com/swiftcsv/SwiftCSV\", from: \"0.5.6\")\n",
      "\t\tSwiftCSV\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmptlwx94hl/swift-install\n",
      "[1/2] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "[2/3] Merging module jupyterInstalledPackages\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "// Swift package installation must go in the first cell.\n",
    "// Set install-location so that all notebooks in this directory\n",
    "// will share packages without having to recompile them.\n",
    "// I think this only needs to be run once per host, then in the future\n",
    "// you can skip this and go to cell 2 which imports things that have been precompiled.\n",
    "%install-location $cwd/swift-jupyter-install-location\n",
    "%install '.package(url: \"https://github.com/mxcl/Path.swift\", from: \"1.2.0\")' Path\n",
    "%install '.package(url: \"https://github.com/swiftcsv/SwiftCSV\", from: \"0.5.6\")' SwiftCSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Imports and Jupyter boiler plate\n",
    "import Foundation\n",
    "import FoundationNetworking\n",
    "import Path\n",
    "import PythonKit\n",
    "import SwiftCSV\n",
    "import TensorFlow\n",
    "\n",
    "// This cell is here to display the plots in a Jupyter Notebook.\n",
    "// Do not copy it into another environment.\n",
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")\n",
    "\n",
    "let plt = Python.import(\"matplotlib.pyplot\")\n",
    "let tarfile = Python.import(\"tarfile\")\n",
    "let zipfile = Python.import(\"zipfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: move this to a library or use SwiftAI\n",
    "func download(from source: URL, to destination: Path, force: Bool = false) {\n",
    "    if (destination.exists && !force) {\n",
    "        return\n",
    "    }\n",
    "    let data = try! Data.init(contentsOf: source)\n",
    "    if (!destination.parent.exists) {\n",
    "        try! destination.parent.mkdir(.p)\n",
    "    }\n",
    "    try! data.write(to: destination)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/garymm_gmail_com/src/fastai/course-v3/nbs/dl1-swift/data/ml-100k.zip\r\n"
     ]
    }
   ],
   "source": [
    "let dataPath = Path.cwd/\"data\"\n",
    "// Full data set: \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "// Sample data set: \"http://files.fast.ai/data/examples/movie_lens_sample.tgz\"\n",
    "let movieLensURL = URL(string: \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\")!\n",
    "let downloadedArchive = dataPath/movieLensURL.lastPathComponent\n",
    "download(from: movieLensURL, to: downloadedArchive)\n",
    "print(downloadedArchive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 90569 training rows, 9429 validation rows\r\n"
     ]
    }
   ],
   "source": [
    "var archiveFile : PythonObject = 1\n",
    "switch downloadedArchive.extension {\n",
    "    case \"tgz\":\n",
    "        archiveFile = tarfile.open(downloadedArchive.string)\n",
    "    case \"zip\":\n",
    "        archiveFile = zipfile.ZipFile(downloadedArchive.string)\n",
    "    default:\n",
    "        print(\"Unknown extension:\", downloadedArchive.extension)\n",
    "}\n",
    "archiveFile.extractall(path: dataPath.string)\n",
    "archiveFile.close()\n",
    "\n",
    "// See extractedDir/README for details on the files and their contents.\n",
    "let userIdColIndex = 0\n",
    "let movieIdColIndex = 1\n",
    "let ratingColIndex = 2\n",
    "\n",
    "let extractedDir = dataPath/downloadedArchive.basename(dropExtension: true)\n",
    "let trainTSV: CSV = CSV(url: URL(fileURLWithPath: (extractedDir/\"ua.base\").string), delimiter: \"\\t\", loadColumns: false)\n",
    "let validTSV: CSV = CSV(url: URL(fileURLWithPath: (extractedDir/\"ua.test\").string), delimiter: \"\\t\", loadColumns: false)\n",
    "print(\"loaded \\(trainTSV.namedRows.count) training rows, \\(validTSV.namedRows.count) validation rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// Examples from the collaborative filtering dataset.\n",
    "struct CollabData {\n",
    "    // [batchSize, featureCount] tensor of features.\n",
    "    let features: Tensor<Int32>\n",
    "\n",
    "    // [batchSize] tensor of labels.\n",
    "    let labels: Tensor<Float>\n",
    "}\n",
    "\n",
    "/// Conform `CollabData` to `Collatable` so that we can load it into a `TrainingEpoch`.\n",
    "extension CollabData: Collatable {\n",
    "    public init<BatchSamples: Collection>(collating samples: BatchSamples)\n",
    "        where BatchSamples.Element == Self {\n",
    "        // `CollabData`s are collated by stacking their feature and label tensors\n",
    "        // along the batch axis to produce a single feature and label tensor\n",
    "        features = Tensor<Int32>(stacking: samples.map{$0.features})\n",
    "        labels = Tensor<Float>(stacking: samples.map{$0.labels})\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "var trainRatingsArray: [Float] = [], validRatingsArray: [Float] = []\n",
    "// UInt16 would be more efficient for this application, but Embedding takes only Int32.\n",
    "var trainIdsArray: [[Int32]] = [], validIdsArray: [[Int32]] = []\n",
    "\n",
    "var maxUserId: Int32 = 0\n",
    "var maxMovieId: Int32 = 0\n",
    "\n",
    "// TODO: this boolean is ugly. Is there a better way to do this?\n",
    "func addToArray(row: [String], isTrain: Bool) {\n",
    "    if row.count < ratingColIndex + 1 {\n",
    "        return\n",
    "    }\n",
    "    guard let userId = Int32(row[userIdColIndex]) else {\n",
    "        print(\"Failed to convert element \\(userIdColIndex) of row to Int32: \\(row)\")\n",
    "        return\n",
    "    }\n",
    "    guard let movieId = Int32(row[movieIdColIndex]) else {\n",
    "        print(\"Failed to convert element \\(movieIdColIndex) of row to Int32: \\(row)\")\n",
    "        return\n",
    "    }\n",
    "    \n",
    "    guard let rating = Float(row[ratingColIndex]) else {\n",
    "        print(\"Failed to convert to Float: \\(row[ratingColIndex])\")\n",
    "        return\n",
    "    }\n",
    "    maxUserId = max(maxUserId, userId)\n",
    "    maxMovieId = max(maxMovieId, movieId)\n",
    "    if isTrain {\n",
    "        trainIdsArray.append([userId, movieId])\n",
    "        trainRatingsArray.append(rating)\n",
    "    } else {\n",
    "        validIdsArray.append([userId, movieId])\n",
    "        validRatingsArray.append(rating)\n",
    "    }\n",
    "}\n",
    "\n",
    "try trainTSV.enumerateAsArray { addToArray(row: $0, isTrain: true) }\n",
    "try validTSV.enumerateAsArray { addToArray(row: $0, isTrain: false) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traingDataset:\n",
      "Row 0: [1, 1] 5.0\n",
      "Row 1: [1, 2] 3.0\n",
      "Row 2: [1, 3] 4.0\n",
      "Row 3: [1, 4] 3.0\n",
      "Row 4: [1, 5] 3.0\n",
      "Row 5: [1, 6] 5.0\n",
      "First batch of features: [[ 785,  748],\n",
      " [ 594,   19],\n",
      " [ 661,  357],\n",
      " [ 271,   88],\n",
      " [ 184,  161],\n",
      " [   7,   69],\n",
      " [ 577,  549],\n",
      " [ 727,  635],\n",
      " [ 851,  109],\n",
      " [ 391,  288],\n",
      " [ 305,   11],\n",
      " [ 497,   73],\n",
      " [ 647,   82],\n",
      " [ 735,  332],\n",
      " [ 524,  928],\n",
      " [ 452,  154],\n",
      " [ 134,  294],\n",
      " [ 913,  210],\n",
      " [ 503,  385],\n",
      " [ 463,  887],\n",
      " [ 833,   28],\n",
      " [ 234, 1269],\n",
      " [ 757,  231],\n",
      " [ 350,  515],\n",
      " [   1,  254],\n",
      " [ 197,  550],\n",
      " [ 645,   96],\n",
      " [ 566,   97],\n",
      " [ 119,  182],\n",
      " [ 429,  629],\n",
      " [ 104,  222],\n",
      " [ 342,   25],\n",
      " [ 363,  423],\n",
      " [ 291,  393],\n",
      " [  18,  425],\n",
      " [ 809,  245],\n",
      " [ 407,  519],\n",
      " [ 346,   55],\n",
      " [ 306, 1514],\n",
      " [ 537,  745],\n",
      " [  82,  230],\n",
      " [ 833,  249],\n",
      " [   2,  303],\n",
      " [ 708,  756],\n",
      " [ 332,  682],\n",
      " [ 509,  294],\n",
      " [ 119,  931],\n",
      " [ 423,  924],\n",
      " [ 881,  472],\n",
      " [ 318, 1014],\n",
      " [ 808,  751],\n",
      " [ 249,  483],\n",
      " [ 585,  170],\n",
      " [ 280,  155],\n",
      " [ 545,  434],\n",
      " [ 345,    4],\n",
      " [  76,  513],\n",
      " [ 319,  682],\n",
      " [ 279,  184],\n",
      " [ 308,  614],\n",
      " [ 297,  574],\n",
      " [ 600,  515],\n",
      " [ 394,   69],\n",
      " [ 630,  640]]\n",
      "firstTrainFeatures.shape: [64, 2]\n",
      "First batch of labels: [3.0, 3.0, 4.0, 4.0, 2.0, 5.0, 5.0, 2.0, 4.0, 3.0, 1.0, 3.0, 4.0, 3.0, 4.0, 5.0, 4.0, 2.0, 1.0, 5.0, 3.0, 3.0, 2.0, 5.0, 1.0, 3.0,\n",
      " 3.0, 3.0, 4.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 4.0, 5.0, 4.0, 2.0, 2.0, 1.0, 4.0, 2.0, 4.0, 2.0, 1.0, 4.0, 4.0, 2.0, 3.0, 5.0,\n",
      " 5.0, 5.0, 3.0, 4.0, 5.0, 3.0, 5.0, 3.0, 1.0, 5.0, 5.0, 1.0]\n",
      "firstTrainLabels.shape: [64]\n"
     ]
    }
   ],
   "source": [
    "// trainDataSet is an array of CollabData, one per example.\n",
    "let trainDataset = zip(trainIdsArray, trainRatingsArray).map {\n",
    "    CollabData(features: Tensor<Int32>($0.0), labels: Tensor<Float>($0.1))\n",
    "}\n",
    "let validDataset = zip(validIdsArray, validRatingsArray).map {\n",
    "    CollabData(features: Tensor<Int32>($0.0), labels: Tensor<Float>($0.1))\n",
    "}\n",
    "\n",
    "print(\"traingDataset:\")\n",
    "for i in 0 ... 5 {\n",
    "    print(\"Row \\(i):\", trainDataset[i].features, trainDataset[i].labels)\n",
    "}\n",
    "\n",
    "let batchSize = 64\n",
    "let trainEpochs = TrainingEpochs(samples: trainDataset, batchSize: batchSize)\n",
    "let validEpochs = TrainingEpochs(samples: validDataset, batchSize: batchSize)\n",
    "\n",
    "let firstTrainEpoch = trainEpochs.next()!\n",
    "let firstTrainBatch = firstTrainEpoch.first!.collated\n",
    "let firstTrainFeatures = firstTrainBatch.features\n",
    "let firstTrainLabels = firstTrainBatch.labels\n",
    "\n",
    "print(\"First batch of features: \\(firstTrainFeatures)\")\n",
    "print(\"firstTrainFeatures.shape: \\(firstTrainFeatures.shape)\")\n",
    "print(\"First batch of labels: \\(firstTrainLabels)\")\n",
    "print(\"firstTrainLabels.shape: \\(firstTrainLabels.shape)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct EmbeddingDotBias: Module {\n",
    "    var uWeight, iWeight, uBias, iBias: Embedding<Float>\n",
    "    \n",
    "    @noDerivative\n",
    "    let yRange: (Float, Float)?\n",
    "    \n",
    "    init(nFactors: Int, nUsers: Int, nItems: Int, yRange: (Float, Float)?) {\n",
    "        self.uWeight = Embedding<Float>(vocabularySize: nUsers, embeddingSize: nFactors)\n",
    "        self.iWeight = Embedding<Float>(vocabularySize: nItems, embeddingSize: nFactors)\n",
    "        self.uBias = Embedding<Float>(vocabularySize: nUsers, embeddingSize: 1)\n",
    "        self.iBias = Embedding<Float>(vocabularySize: nItems, embeddingSize: 1)\n",
    "        self.yRange = yRange\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func callAsFunction(_ input: Tensor<Int32>) -> Tensor<Float> {\n",
    "        let inputParts = input.split(count: 2, alongAxis: 1)\n",
    "        let users = inputParts[userIdColIndex].squeezingShape()\n",
    "        let items = inputParts[movieIdColIndex].squeezingShape()\n",
    "        let dotProds = (uWeight(users) * iWeight(items)).sum(alongAxes: 1).squeezingShape()\n",
    "        let res = dotProds + uBias(users).squeezingShape() + iBias(items).squeezingShape()\n",
    "        \n",
    "        guard let (yMin, yMax) = yRange else {\n",
    "            return res\n",
    "        }\n",
    "        return sigmoid(res) * (yMax - yMin) + yMin\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set the yRange slightly larger than the actual range of ratings because sigmoid approaches\n",
    "// the limits asymptotically.\n",
    "var model = EmbeddingDotBias(nFactors: 16, nUsers: Int(maxUserId), nItems: Int(maxMovieId), yRange: (-0.25, 5.25))\n",
    "let optimizer = Adam(for: model, learningRate: 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "let epochCount = 1\n",
    "var trainLosses: [Tensor<Float>] = []\n",
    "var validLosses: [Tensor<Float>] = []\n",
    "var trainBatchCount, validBatchCount: Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with epoch: 0\r\n"
     ]
    }
   ],
   "source": [
    "for (epochIndex, epoch) in trainEpochs.prefix(epochCount).enumerated() {\n",
    "    trainBatchCount = 0; validBatchCount = 0\n",
    "    for batchSamples in epoch {\n",
    "        let batch = batchSamples.collated\n",
    "        let (loss, grad) = valueWithGradient(at: model) { (model: EmbeddingDotBias) -> Tensor<Float> in\n",
    "            return meanSquaredError(predicted: model(batch.features), expected: batch.labels)\n",
    "        }\n",
    "        optimizer.update(&model, along: grad)        \n",
    "        trainLosses.append(loss)\n",
    "        trainBatchCount += 1\n",
    "    }\n",
    "    let validSamples = validEpochs.next()!\n",
    "    for batchSamples in validSamples {\n",
    "        let batch = batchSamples.collated\n",
    "        let loss = meanSquaredError(predicted: model(batch.features), expected: batch.labels)\n",
    "        validLosses.append(loss)\n",
    "        validBatchCount += 1\n",
    "    }\n",
    "    print(\"Done with epoch:\", epochIndex)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainLossesPerEpoch: [0.82688904]\r\n",
      "validLossesPerEpoch: [7.95951]\r\n"
     ]
    }
   ],
   "source": [
    "var trainLossesPerEpoch: [Float] = []\n",
    "var validLossesPerEpoch: [Float] = []\n",
    "for i in 0..<epochCount {\n",
    "    var trainLoss: Float = 0\n",
    "    var validLoss: Float = 0\n",
    "    for b in 0..<trainBatchCount {\n",
    "        trainLoss += trainLosses[b].scalarized()\n",
    "    }\n",
    "    for b in 0..<validBatchCount {\n",
    "        validLoss += validLosses[b].scalarized()\n",
    "    }\n",
    "    trainLossesPerEpoch.append(trainLoss / Float(trainBatchCount))\n",
    "    validLossesPerEpoch.append(trainLoss / Float(validBatchCount))\n",
    "}\n",
    "print(\"trainLossesPerEpoch:\", trainLossesPerEpoch)\n",
    "print(\"validLossesPerEpoch:\", validLossesPerEpoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let validDataset = zip(validIdsArray, validRatingsArray).map {\n",
    "    CollabData(features: Tensor<Int32>($0.0), labels: Tensor<Float>($0.1))\n",
    "}\n",
    "print(\"validation dataset size:\", validDataset.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: fastai has a nice way of computing the loss on the entire validation set.\n",
    "// Figure out how that's done rather than re-inventing mean squared error here.\n",
    "var squaredError: Double = 0\n",
    "\n",
    "for batchSamples in validDataset.inBatches(of: batchSize) {\n",
    "    let batch = batchSamples.collated\n",
    "    squaredError += Double(squaredDifference(model(batch.features), batch.labels).sum().scalarized())\n",
    "}\n",
    "\n",
    "let meanSquaredError = squaredError / Double(validDataset.count)\n",
    "print(\"meanSquaredError on validation set:\", meanSquaredError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO:\n",
    "// * Try this out on the full data set and see if performance is better\n",
    "// * Get the movie names and inspect the movie biases\n",
    "// * Visualize embeddings with principle component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
